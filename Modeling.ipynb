{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"9aIXX3Sy0iPU"},"outputs":[],"source":["import os\n","import glob\n","\n","import torch\n","import torchvision\n","import cv2\n","from torchvision import models\n","import torch.nn as nn\n","from torch.autograd import Variable\n","\n","from google.colab import drive\n","from google.colab.patches import cv2_imshow"]},{"cell_type":"code","source":["drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aWJqhHOz1S-z","executionInfo":{"status":"ok","timestamp":1668723333973,"user_tz":300,"elapsed":17203,"user":{"displayName":"Sri Dittakavi","userId":"06169176449632754410"}},"outputId":"0da651e9-30c1-41b9-87bb-b304efb1bcbb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["os.chdir('/content/gdrive/My Drive/lipreading')"],"metadata":{"id":"LT2x2iEE1ZYh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Lets investigate pre-train, where we have word annotations with time, \n","# which will probably be necessary for training the model accurately\n","\n","pretrain_paths = glob.glob('data/pretrain/*/*.mp4')\n","pretrain_transcripts = [x.replace('.mp4', '.txt') for x in pretrain_paths]"],"metadata":{"id":"4B2u2Ylm0xWQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_frames(video_path, desired_frames=None):\n","  video_cap = cv2.VideoCapture(video_path)\n","\n","  fps = int(video_cap.get(5))\n","  frame_count = int(video_cap.get(7))\n","  \n","  if desired_frames:\n","    if desired_frames > frame_count:\n","      print(f'Requested {desired_frames} frames, the video only contains {frame_count} frames')\n","      return []\n","\n","  print('FPS: ', fps)\n","  print('Frames: ', frame_count)\n","  print(f'Duration: {(frame_count/fps)}s')\n","\n","  frames = []\n","\n","  frame_limit = frame_count if not desired_frames else desired_frames\n","  for i in range(int(frame_limit)):\n","    ret, val = video_cap.read()\n","\n","    if ret:\n","      frames.append(val)\n","    else:\n","      print('Error occured')\n","      break\n","\n","  video_cap.release()\n","  return frames"],"metadata":{"id":"BSIm0Hyp2Eam"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pretrain_paths[0], pretrain_transcripts[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9GuvNevb2LNP","executionInfo":{"status":"ok","timestamp":1668724511538,"user_tz":300,"elapsed":184,"user":{"displayName":"Sri Dittakavi","userId":"06169176449632754410"}},"outputId":"71a27576-b638-4696-9e05-1fe79ba4e9ff"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('data/pretrain/ZzSVEj5RLWM/00005.mp4', 'data/pretrain/ZzSVEj5RLWM/00005.txt')"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["frames = get_frames(pretrain_paths[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RWdBcOH62Non","executionInfo":{"status":"ok","timestamp":1668724639127,"user_tz":300,"elapsed":399,"user":{"displayName":"Sri Dittakavi","userId":"06169176449632754410"}},"outputId":"af6350c5-18da-4e5d-8504-9c738380c3f9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["FPS:  25\n","Frames:  692\n","Duration: 27.68s\n"]}]},{"cell_type":"code","source":["# This will let us know what the lowest number of frames is in the videos and the location in the array\n","def findLowestNumFrames():\n","  numFrames = []\n","\n","  for i in range(len(pretrain_paths)):\n","    frames = get_frames(pretrain_paths[i])\n","    numFrames.append(len(frames))\n","\n","  print(\"Lowest Number of Frames in All Videos: \" + str( min(numFrames)))\n","  print(\"Index of the Lowest Number of Frames in All Videos: \" + str(numFrames.index(min(numFrames))))\n","\n","findLowestNumFrames() "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HcuLCurJvYtW","executionInfo":{"status":"ok","timestamp":1668728831340,"user_tz":300,"elapsed":1443,"user":{"displayName":"Sri Dittakavi","userId":"06169176449632754410"}},"outputId":"b5c0a0fb-2034-4ca4-942b-42917cd27866"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["FPS:  25\n","Frames:  692\n","Duration: 27.68s\n","FPS:  25\n","Frames:  1117\n","Duration: 44.68s\n","FPS:  25\n","Frames:  1184\n","Duration: 47.36s\n","FPS:  25\n","Frames:  221\n","Duration: 8.84s\n","FPS:  25\n","Frames:  204\n","Duration: 8.16s\n","Lowest Number of Frames in All Videos: 204\n","Index of the Lowest Number of Frames in All Videos: 4\n"]}]},{"cell_type":"code","source":["with open(pretrain_transcripts[0], 'r') as f:\n","  s = f.read()\n","\n","print(s)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uYGl7qtO2RJx","executionInfo":{"status":"ok","timestamp":1668550529901,"user_tz":300,"elapsed":306,"user":{"displayName":"Cade Mack","userId":"05278390605828414866"}},"outputId":"23b2cb4a-c6eb-4643-8c4c-5ee870b8ced9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Text:  OUTFIT I'M PASSIONATE ABOUT CHANGING THE FASHION INDUSTRY ABOUT PROVIDING SUSTAINABLE CLOTHING OPTIONS AND SUPPORTING DESIGNERS WHO ARE DOING THE RIGHT THING EVERYTHING YOU ARE WEARING TODAY HAS A STORY WITH CONTRIBUTORS AND CHANGE COMES FROM PEOPLE LIKE YOU THE EVERYDAY CONSUMERS OF CLOTHING AND TOGETHER WE CAN CHANGE THE FASHION INDUSTRY \n","Conf:  4\n","\n","WORD START END ASDSCORE\n","OUTFIT 0.08 0.58 4.5\n","I'M 4.05 4.38 5.0\n","PASSIONATE 4.38 4.86 3.8\n","ABOUT 4.86 5.20 2.7\n","CHANGING 5.20 5.65 2.6\n","THE 5.65 5.78 1.8\n","FASHION 5.78 6.14 4.4\n","INDUSTRY 6.14 6.69 0.5\n","ABOUT 7.24 7.54 2.2\n","PROVIDING 7.54 8.10 4.9\n","SUSTAINABLE 8.10 8.71 5.3\n","CLOTHING 8.71 9.04 7.0\n","OPTIONS 9.04 9.55 3.9\n","AND 9.93 10.04 13.0\n","SUPPORTING 10.04 10.57 8.4\n","DESIGNERS 10.57 11.08 4.1\n","WHO 11.08 11.28 3.6\n","ARE 11.28 11.35 2.9\n","DOING 11.35 11.79 2.7\n","THE 11.79 11.90 3.0\n","RIGHT 11.90 12.22 6.1\n","THING 12.25 12.56 2.1\n","EVERYTHING 13.59 14.15 2.6\n","YOU 14.15 14.23 3.5\n","ARE 14.23 14.36 6.7\n","WEARING 14.36 14.67 4.9\n","TODAY 14.67 15.38 3.9\n","HAS 15.78 16.00 8.0\n","A 16.00 16.03 6.5\n","STORY 16.03 16.59 5.2\n","WITH 17.17 17.42 4.2\n","CONTRIBUTORS 17.42 18.17 2.5\n","AND 18.95 19.17 7.5\n","CHANGE 19.17 19.66 5.9\n","COMES 19.66 19.90 3.8\n","FROM 19.90 20.08 2.8\n","PEOPLE 20.08 20.39 3.6\n","LIKE 20.39 20.74 6.8\n","YOU 20.74 21.17 3.8\n","THE 21.45 21.61 5.4\n","EVERYDAY 21.61 22.23 4.5\n","CONSUMERS 22.23 22.76 3.9\n","OF 22.76 22.89 2.2\n","CLOTHING 22.89 23.38 1.0\n","AND 24.16 24.35 1.2\n","TOGETHER 24.35 25.01 1.8\n","WE 25.32 25.53 6.2\n","CAN 25.56 25.99 6.7\n","CHANGE 25.99 26.49 5.1\n","THE 26.49 26.57 4.8\n","FASHION 26.57 26.98 5.0\n","INDUSTRY 26.98 27.54 0.5\n","\n"]}]},{"cell_type":"code","source":["frames[0].shape"],"metadata":{"id":"w8R83HWwK9cJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["example_length_seconds = 1\n","fps = 25\n","batch_size = 2\n","width, height = 224, 224\n","channels = 3\n","\n","\n","data_shape = (batch_size, fps*example_length_seconds, channels, width, height)\n","\n","fake_data = Variable(torch.randint(0, 255, data_shape)).float()\n","fake_data.shape"],"metadata":{"id":"HXtAEXtZM7-8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668553519786,"user_tz":300,"elapsed":278,"user":{"displayName":"Cade Mack","userId":"05278390605828414866"}},"outputId":"92f588f8-7b2b-454d-8d81-d3d86df4f203"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 25, 3, 224, 224])"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["resnet = models.resnet50(pretrained=True)"],"metadata":{"id":"WS9tSsSMRl7j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class LREncoder(nn.Module):\n","  def __init__(self, params):\n","    super(LREncoder,self).__init__()  \n","\n","    dr_rate= params[\"dr_rate\"]\n","    hidden_size = params[\"hidden_size\"]\n","    rnn_num_layers = params[\"rnn_num_layers\"]\n","    embedding_dim = params[\"embedding_dim\"]\n","      \n","    basemodel = models.alexnet(pretrained=True)\n","    basemodel.fc = nn.Linear(in_features=2048, out_features=embedding_dim, bias=True)\n","    self.basemodel = basemodel\n","\n","    self.dropout= nn.Dropout(dr_rate)\n","    # self.encoder = nn.Transformer(d_model=embedding_dim)\n","    self.encoder = nn.GRU(input_size=embedding_dim, hidden_size=hidden_size, batch_first=True)\n","\n","  def features(self, x):\n","    bs, ts, c, h, w = x.shape\n","    ys = []\n","    for ii in range(0, ts):\n","      yi = self.basemodel((x[:,ii]))\n","      ys.append(yi)\n","    ys = torch.stack(ys, 1)\n","    return ys\n","\n","  def forward(self, x):\n","    ys = self.features(x)\n","    y = self.encoder(ys)\n","    return y\n"],"metadata":{"id":"5SxIdDWkLYFA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_params = {}\n","model_params['dr_rate'] = 0.1\n","model_params['hidden_size'] = 32\n","model_params['rnn_num_layers'] = 1\n","model_params['embedding_dim'] = 1000\n","\n","model = LREncoder(model_params)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2kKpFTWtUaTE","executionInfo":{"status":"ok","timestamp":1668555174658,"user_tz":300,"elapsed":1094,"user":{"displayName":"Cade Mack","userId":"05278390605828414866"}},"outputId":"df3ebae8-4687-4c10-c648-9952b8530d0b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]}]},{"cell_type":"code","source":["print(fake_data.shape)\n","fake_ys = model.features(fake_data)\n","print(fake_ys.shape)\n","fake_y = model.encoder(fake_ys)\n","for item in fake_y:\n","  print(item.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"--DMiu8hg-ox","executionInfo":{"status":"ok","timestamp":1668555179105,"user_tz":300,"elapsed":2889,"user":{"displayName":"Cade Mack","userId":"05278390605828414866"}},"outputId":"92d77415-8000-42f2-9251-01fd1d348f34"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 25, 3, 224, 224])\n","torch.Size([2, 25, 1000])\n","torch.Size([2, 25, 32])\n","torch.Size([1, 2, 32])\n"]}]},{"cell_type":"code","source":["rnn = nn.GRU(10, 20, 2)\n","input = torch.randn(5, 3, 10)\n","h0 = torch.randn(2, 3, 20)\n","output, hn = rnn(input, h0)\n","output.shape, hn.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vIctUHDhn_uP","executionInfo":{"status":"ok","timestamp":1668555269959,"user_tz":300,"elapsed":278,"user":{"displayName":"Cade Mack","userId":"05278390605828414866"}},"outputId":"44044eb3-e404-42ab-c56a-37fe9a330c2a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([5, 3, 20]), torch.Size([2, 3, 20]))"]},"metadata":{},"execution_count":54}]},{"cell_type":"code","source":["# model.resnet(Variable(torch.randint(0, 255, (1, 3, 224, 224))).float())\n","fake_y = model(Variable(fake_data).float())\n","fake_data.shape, fake_y.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eHk91n88VOz1","executionInfo":{"status":"ok","timestamp":1668553594336,"user_tz":300,"elapsed":2627,"user":{"displayName":"Cade Mack","userId":"05278390605828414866"}},"outputId":"c3adc85b-7dd0-4470-e67b-7208809eb67a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([2, 25, 3, 224, 224]), torch.Size([2, 24, 1000]))"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["class Identity(nn.Module):\n","    def __init__(self):\n","        super(Identity, self).__init__()\n","    def forward(self, x):\n","        return x  \n","\n","class Resnt18Rnn(nn.Module):\n","    def __init__(self, params_model):\n","        super(Resnt18Rnn, self).__init__()\n","        num_classes = params_model[\"num_classes\"]\n","        dr_rate = params_model[\"dr_rate\"]\n","        pretrained = params_model[\"pretrained\"]\n","        rnn_hidden_size = params_model[\"rnn_hidden_size\"]\n","        rnn_num_layers = params_model[\"rnn_num_layers\"]\n","        \n","        baseModel = models.resnet18(pretrained=pretrained)\n","        num_features = baseModel.fc.in_features\n","        baseModel.fc = Identity()\n","        self.baseModel = baseModel\n","        self.dropout= nn.Dropout(dr_rate)\n","        self.rnn = nn.LSTM(num_features, rnn_hidden_size, rnn_num_layers)\n","        self.fc1 = nn.Linear(rnn_hidden_size, num_classes)\n","  \n","    def forward(self, x):\n","        b_z, ts, c, h, w = x.shape\n","        ii = 0\n","        y = self.baseModel((x[:,ii]))\n","        output, (hn, cn) = self.rnn(y.unsqueeze(1))\n","        for ii in range(1, ts):\n","            y = self.baseModel((x[:,ii]))\n","            out, (hn, cn) = self.rnn(y.unsqueeze(1), (hn, cn))\n","        out = self.dropout(out[:,-1])\n","        out = self.fc1(out) \n","        print(hn.shape)\n","        return out "],"metadata":{"id":"91gMJ5Z0Sfqr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["params_model = {}\n","params_model[\"num_classes\"] = 10\n","params_model[\"dr_rate\"] = 0.1\n","params_model[\"pretrained\"] = 'pretrained'\n","params_model[\"rnn_hidden_size\"] = 32\n","params_model[\"rnn_num_layers\"] = 1\n","\n","model = Resnt18Rnn(params_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zmTrczrIfDjD","executionInfo":{"status":"ok","timestamp":1668552867354,"user_tz":300,"elapsed":312,"user":{"displayName":"Cade Mack","userId":"05278390605828414866"}},"outputId":"e354746c-5720-47da-db50-c3306c0ec92a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]}]},{"cell_type":"code","source":["fake_y = model(Variable(fake_data).float())\n","fake_y.shape, fake_data.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NRYgXaEXfmEj","executionInfo":{"status":"ok","timestamp":1668552874595,"user_tz":300,"elapsed":4160,"user":{"displayName":"Cade Mack","userId":"05278390605828414866"}},"outputId":"f95e7bf6-00fa-40bb-b2ed-0fe5b5f7efa7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 1, 32])\n"]},{"output_type":"execute_result","data":{"text/plain":["(torch.Size([2, 10]), torch.Size([2, 25, 3, 224, 224]))"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":[],"metadata":{"id":"sKZ-QKYhGVev"},"execution_count":null,"outputs":[]}]}